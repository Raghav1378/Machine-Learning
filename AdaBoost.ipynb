{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9024a35-58f1-4de3-ae73-013d4e6b3ca8",
   "metadata": {},
   "source": [
    "# Weak Learners -Model that are very less in accuracy and the model accuracy is somewhere around 50%. \n",
    "# Deiscion stumps are type of weak learner in that we can only split it in 1 node which means there is only 1 depth\n",
    "# SO baiscally Desicion stumps is decision tree that has only one level "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f5792-bcbc-4ac6-a43c-888e0672ba30",
   "metadata": {},
   "source": [
    "# \"AdaBoost is like a student correcting their mistakes at every exam, learning from the wrong questions, and eventually acing the final combined test.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553d24a-e429-4892-bfae-9e245b4660e9",
   "metadata": {},
   "source": [
    "# Of course! I'll explain **AdaBoost** properly in **bullet points** (suitable for a **Markdown cell**) and cover all **basic topics** you should know, including a **real-life example**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# AdaBoost (Adaptive Boosting)\n",
    "\n",
    "- **What is AdaBoost?**  \n",
    "  AdaBoost (short for *Adaptive Boosting*) is an ensemble learning technique that combines multiple **weak learners** to create a **strong learner**.\n",
    "\n",
    "- **What is a Weak Learner?**  \n",
    "  A weak learner is a model that performs slightly better than random guessing (e.g., a decision stump ‚Äî a one-level decision tree).\n",
    "\n",
    "- **Main Idea of AdaBoost:**  \n",
    "  - Focus more on the samples that are **misclassified**.\n",
    "  - Each new weak model tries to correct the mistakes made by previous models.\n",
    "  - Final prediction is based on the **weighted vote** of all weak models.\n",
    "\n",
    "- **How AdaBoost Works (Step-by-Step):**\n",
    "  1. Start by assigning **equal weights** to all training samples.\n",
    "  2. Train a weak learner (e.g., decision stump).\n",
    "  3. Check the **errors** (misclassifications).\n",
    "  4. Increase the **weights of misclassified points** so that the next model focuses more on them.\n",
    "  5. Train another weak learner using the updated weights.\n",
    "  6. Repeat for a fixed number of iterations or until error is minimized.\n",
    "  7. Combine all weak learners by giving **higher importance** to better-performing models.\n",
    "\n",
    "- **Important Concepts:**\n",
    "  - **Error Rate:** Proportion of samples misclassified by a weak learner.\n",
    "  - **Alpha (Model Weight):** Determines how much say a weak learner has in the final prediction (lower error ‚Üí higher alpha).\n",
    "  - **Weighted Majority Vote:** In classification, final decision is a weighted vote of all models based on their alpha.\n",
    "\n",
    "- **Why Use AdaBoost?**\n",
    "  - Reduces bias and variance.\n",
    "  - Works well even if individual models are weak.\n",
    "  - Automatically focuses on hard examples.\n",
    "\n",
    "- **Common Weak Learner Used:**\n",
    "  - Simple Decision Trees (Decision Stumps)\n",
    "\n",
    "- **Strengths of AdaBoost:**\n",
    "  - Easy to implement.\n",
    "  - Often very accurate.\n",
    "  - No need to tweak many parameters (besides number of learners).\n",
    "\n",
    "- **Limitations of AdaBoost:**\n",
    "  - Sensitive to **noisy data** and **outliers** (because it tries very hard to correct misclassified points).\n",
    "  - Needs clean and relevant data.\n",
    "\n",
    "- **Real-Life Scenario:**  \n",
    "  **Face Detection in Images**\n",
    "  - AdaBoost was used in the famous **Viola-Jones Face Detection Algorithm**.\n",
    "  - It combined many simple classifiers to quickly and accurately detect faces in photos and videos.\n",
    "  - Each weak classifier focused on detecting simple patterns (like edges or textures), and together they built a strong model capable of finding faces.\n",
    "\n",
    "- **Summary in One Line:**  \n",
    "  > AdaBoost builds a strong classifier by sequentially combining weak classifiers and focusing on mistakes!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b84d42-df23-4189-870e-784bfc4bc635",
   "metadata": {},
   "source": [
    "# Why to use the weak learner even if the suck \n",
    "## Why weak learners? Because they are fast to train, and they focus on different parts of the data. A weak learner is like a single person in a team ‚Äî not the best, but the team grows stronger when combined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5e503-77b1-4117-899c-273dc4d8b7de",
   "metadata": {},
   "source": [
    "# üêæ How Do Weak Learners Learn From Past Mistakes?\n",
    "## Now, the magic happens in the way AdaBoost adjusts the weights of the data based on the mistakes each learner makes. Here‚Äôs how:\n",
    "\n",
    "## First Learner:\n",
    "\n",
    "## The first weak learner is trained on the whole dataset (with equal weights for all data points).\n",
    "\n",
    "## It tries to predict the target (e.g., class 0 or class 1).\n",
    "\n",
    "## It makes some mistakes and gets certain points wrong.\n",
    "\n",
    "## Weight Adjustment:\n",
    "\n",
    "## After the first learner finishes, AdaBoost increases the weights of the misclassified points ‚Äî these are the points the model got wrong.\n",
    "\n",
    "## The idea is to tell the next learner to pay more attention to these \"hard\" cases ‚Äî the mistakes the first learner made.\n",
    "\n",
    "## Second Learner:\n",
    "\n",
    "## The second weak learner is trained, but this time, it focuses more on the mistakes made by the first learner because those points have higher weights.\n",
    "\n",
    "## It tries to correct those errors.\n",
    "\n",
    "## Repeat:\n",
    "\n",
    "## This process is repeated for several learners (AdaBoost uses a lot of them). Each new learner focuses on correcting the mistakes of the previous ones.\n",
    "\n",
    "## Combining All Learners:\n",
    "\n",
    "## After all the weak learners are trained, AdaBoost combines their outputs.\n",
    "\n",
    "## It does this using weighted voting, where the learners that did better on the task have more influence in the final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e6099-9fd4-4821-9e76-fe30fb0a6e6f",
   "metadata": {},
   "source": [
    "#     üöÄ Start\n",
    "  #  ‚¨áÔ∏è\n",
    "# üéØ Train Weak Learner 1 (equal weights)\n",
    " #  ‚¨áÔ∏è\n",
    "# ‚ùå Identify Misclassified Points\n",
    "  # ‚¨áÔ∏è\n",
    "# üìà Increase Weights for Misclassified Points\n",
    "  # ‚¨áÔ∏è\n",
    "# üéØ Train Weak Learner 2 (focused on hard samples)\n",
    "  # ‚¨áÔ∏è\n",
    "# üîÅ Repeat Process for More Learners\n",
    "  # ‚¨áÔ∏è\n",
    "# üß† Combine All Weak Learners (weighted voting)\n",
    "  # ‚¨áÔ∏è\n",
    "# üèÜ Final Strong Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0825778a-3859-41bd-aa04-91e6921c66d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91800\\Desktop\\vs codeee\\venv\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost Model: 0.83\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Create a simple dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "                            n_redundant=5, random_state=42)\n",
    "\n",
    "# Step 2: Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Define a weak learner (Decision Stump)\n",
    "weak_learner = DecisionTreeClassifier(max_depth=1)  # Depth 1 to create a weak learner\n",
    "\n",
    "# Step 4: Build the AdaBoost model\n",
    "model = AdaBoostClassifier(estimator=weak_learner, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Step 5: Train the model (AdaBoost will focus on mistakes each round)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of AdaBoost Model: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60812eb6-2c2b-4969-9a05-2875ad0e704d",
   "metadata": {},
   "source": [
    "# Why to use the Desicion Stumps in Adaboost? \n",
    "## Fast to train and simple (avoiding overfitting).\n",
    "\n",
    "## Keeps AdaBoost‚Äôs process stable by making small corrections.\n",
    "\n",
    "## Easier to combine multiple simple learners compared to complex ones.\n",
    "\n",
    "## Lower risk of overfitting compared to complex learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cb637b-bfcf-496d-ba56-25b6ec62921b",
   "metadata": {},
   "source": [
    "# THe core Idea of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3428b3-df0f-4778-bea3-97f1a2e0aea2",
   "metadata": {},
   "source": [
    "# Adaboost works by training multiple weak learners (typically simple models like decision stumps). After training the first model, we evaluate its performance and note the errors. In the next iteration, a second model is trained, but with a focus on misclassified samples from the previous model. These samples are given higher weights so that the new model will pay more attention to correcting the previous mistakes.\n",
    "\n",
    "# This process is repeated for n iterations, where each new model adapts to correct the errors of the previous one. Finally, the predictions of all models are combined using a weighted majority vote (for classification) or a weighted sum (for regression), with each model contributing based on its performance. The result is a strong ensemble model that performs better than any individual weak learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d4003-faf4-4a40-b89d-1198c669e903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
